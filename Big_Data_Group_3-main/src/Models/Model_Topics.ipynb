{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import pytz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, udf, lit, pandas_udf, PandasUDFType\n",
    "from sparknlp.pretrained import PretrainedPipeline \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "import emojis\n",
    "from translate import Translator\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "from sparknlp.pretrained import PretrainedPipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/wouterdewitte/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/wouterdewitte/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp-m1_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-131c7ecd-2208-4332-a236-70c36f5d8f56;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp-m1_2.12;4.0.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-m1_2.12;0.4.2 in central\n",
      ":: resolution report :: resolve 230ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp-m1_2.12;4.0.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-m1_2.12;0.4.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-131c7ecd-2208-4332-a236-70c36f5d8f56\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 10:55:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 10:55:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Spark NLP version\n",
      "Apache Spark version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp \n",
    "\n",
    "spark = sparknlp.start(m1=True)\n",
    "\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "findspark.init(\"/Users/wouterdewitte/spark/\")\n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "#sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "#spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this path to your path, for some reason I have an error \n",
    "#reading in all the files\n",
    "#path_json = \".././../data/Topic_vegan/*.json\"\n",
    "\n",
    "# use this if you want all the tweet files, but this is usually too large\n",
    "#df_json = spark.read.json(path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 10:56:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1827680"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_brands = [\"healthyfood\",\n",
    "               \"healthylifestyle\",\n",
    "               \"vegan\",\n",
    "               \"keto\",\n",
    "               \"ketodiet\",\n",
    "               \"ketolifestyle\",\n",
    "               \"veganism\",\n",
    "               \"vegetarian\"]\n",
    "from re import search\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \".././../data/Topic/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "\n",
    "\n",
    "#filter on correct files via keyword\n",
    "files_brand = [file for file in tweet_files if (file.find(list_brands[2]) != -1)]\n",
    "files_brand               \n",
    "               \n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(files_brand)  \n",
    "df_json.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>followers_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>≈ü·ë≠ùêÑcŒπ‚í∫ùî∞ÔΩÅÔº≠ùî≤ùê¨ÔΩÖ·µà ü¶áüå≥üêíüê¥üêõ</td>\n",
       "      <td>speciesamused</td>\n",
       "      <td>Tue Sep 13 22:32:32 +0000 2022</td>\n",
       "      <td>RT @animalsavemvmt: Do you see us? Will you he...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>1426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Camuka üáπüá∑</td>\n",
       "      <td>Zomorok</td>\n",
       "      <td>Tue Sep 13 22:32:26 +0000 2022</td>\n",
       "      <td>RT @angie_karan: #vegan \\n     for the animals...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1224</td>\n",
       "      <td>1386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael Belton</td>\n",
       "      <td>38Belton</td>\n",
       "      <td>Tue Sep 13 22:32:26 +0000 2022</td>\n",
       "      <td>RT @MyVegan_Reach: Cows are forcibly impregnat...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stare Decisis ‚ìã</td>\n",
       "      <td>do_nothing_dem</td>\n",
       "      <td>Tue Sep 13 22:32:16 +0000 2022</td>\n",
       "      <td>RT @angie_karan: #vegan \\n     for the animals...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1224</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mally</td>\n",
       "      <td>mizzishyde</td>\n",
       "      <td>Tue Sep 13 22:32:09 +0000 2022</td>\n",
       "      <td>RT @angie_karan: #vegan \\n     for the animals...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1224</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hiedra-vegan</td>\n",
       "      <td>vegan02131055</td>\n",
       "      <td>Tue Sep 13 22:32:07 +0000 2022</td>\n",
       "      <td>RT @Dodo_Tribe: Jared Leto - \"No More Pus\"  #G...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RadioFreeKrsna</td>\n",
       "      <td>JFave5</td>\n",
       "      <td>Tue Sep 13 22:31:54 +0000 2022</td>\n",
       "      <td>RT @veganrecipebowl: This recipe is also #vega...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VerdeVetriolo@gingerzoerescueranch</td>\n",
       "      <td>VerdeVetriolo</td>\n",
       "      <td>Tue Sep 13 22:31:54 +0000 2022</td>\n",
       "      <td>RT @animalsavemvmt: Do you see us? Will you he...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lightspeed</td>\n",
       "      <td>LightspeedSteps</td>\n",
       "      <td>Tue Sep 13 22:31:49 +0000 2022</td>\n",
       "      <td>RT @DanielleAnd15: #vegan https://t.co/a1eSrYlq23</td>\n",
       "      <td>und</td>\n",
       "      <td>0</td>\n",
       "      <td>287</td>\n",
       "      <td>1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kaz5thlife</td>\n",
       "      <td>kaz6thlife</td>\n",
       "      <td>Tue Sep 13 22:31:49 +0000 2022</td>\n",
       "      <td>Âà∫ÊøÄÁöÑ„Å†„Åë„Å©‚Ä•ÂãïÁâ©Â•Ω„Åç„Å™„Çâ„ÄÅÂøÖ„ÅöÁêÜËß£„Åô„ÇãÁîªÂÉè„Åß„Åô„ÄÇ„Çà„Å≠‚ùóÔ∏è#vegan „ÅÆÊ∞óÊåÅ„Å°„ÅØËâØ„ÅèÁêÜËß£„Åß...</td>\n",
       "      <td>ja</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name      screen_name  \\\n",
       "0                 ≈ü·ë≠ùêÑcŒπ‚í∫ùî∞ÔΩÅÔº≠ùî≤ùê¨ÔΩÖ·µà ü¶áüå≥üêíüê¥üêõ    speciesamused   \n",
       "1                           Camuka üáπüá∑          Zomorok   \n",
       "2                      Michael Belton         38Belton   \n",
       "3                     Stare Decisis ‚ìã   do_nothing_dem   \n",
       "4                               mally       mizzishyde   \n",
       "5                        hiedra-vegan    vegan02131055   \n",
       "6                      RadioFreeKrsna           JFave5   \n",
       "7  VerdeVetriolo@gingerzoerescueranch    VerdeVetriolo   \n",
       "8                          Lightspeed  LightspeedSteps   \n",
       "9                          kaz5thlife       kaz6thlife   \n",
       "\n",
       "                       created_at  \\\n",
       "0  Tue Sep 13 22:32:32 +0000 2022   \n",
       "1  Tue Sep 13 22:32:26 +0000 2022   \n",
       "2  Tue Sep 13 22:32:26 +0000 2022   \n",
       "3  Tue Sep 13 22:32:16 +0000 2022   \n",
       "4  Tue Sep 13 22:32:09 +0000 2022   \n",
       "5  Tue Sep 13 22:32:07 +0000 2022   \n",
       "6  Tue Sep 13 22:31:54 +0000 2022   \n",
       "7  Tue Sep 13 22:31:54 +0000 2022   \n",
       "8  Tue Sep 13 22:31:49 +0000 2022   \n",
       "9  Tue Sep 13 22:31:49 +0000 2022   \n",
       "\n",
       "                                           full_text lang  favorite_count  \\\n",
       "0  RT @animalsavemvmt: Do you see us? Will you he...   en               0   \n",
       "1  RT @angie_karan: #vegan \\n     for the animals...   en               0   \n",
       "2  RT @MyVegan_Reach: Cows are forcibly impregnat...   en               0   \n",
       "3  RT @angie_karan: #vegan \\n     for the animals...   en               0   \n",
       "4  RT @angie_karan: #vegan \\n     for the animals...   en               0   \n",
       "5  RT @Dodo_Tribe: Jared Leto - \"No More Pus\"  #G...   en               0   \n",
       "6  RT @veganrecipebowl: This recipe is also #vega...   en               0   \n",
       "7  RT @animalsavemvmt: Do you see us? Will you he...   en               0   \n",
       "8  RT @DanielleAnd15: #vegan https://t.co/a1eSrYlq23  und               0   \n",
       "9  Âà∫ÊøÄÁöÑ„Å†„Åë„Å©‚Ä•ÂãïÁâ©Â•Ω„Åç„Å™„Çâ„ÄÅÂøÖ„ÅöÁêÜËß£„Åô„ÇãÁîªÂÉè„Åß„Åô„ÄÇ„Çà„Å≠‚ùóÔ∏è#vegan „ÅÆÊ∞óÊåÅ„Å°„ÅØËâØ„ÅèÁêÜËß£„Åß...   ja               0   \n",
       "\n",
       "   retweet_count  followers_count  \n",
       "0             37             1426  \n",
       "1           1224             1386  \n",
       "2              7              375  \n",
       "3           1224              425  \n",
       "4           1224              146  \n",
       "5              9                4  \n",
       "6              4              164  \n",
       "7             37              564  \n",
       "8            287             1414  \n",
       "9              0            10439  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select interesting features\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = df_json.select(F.col(\"user.name\"),\n",
    "                    F.col(\"user.screen_name\"),\n",
    "                    F.col(\"created_at\"), \n",
    "                    F.col(\"full_text\"),\n",
    "                    F.col(\"lang\"),\n",
    "                    F.col(\"favorite_count\"),\n",
    "                    F.col(\"retweet_count\"),\n",
    "                    F.col(\"user.followers_count\"))\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDF‚Äôs are used to extend the functions of the framework and re-use these functions on multiple DataFrame‚Äôs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.twitter.com/en/docs/twitter-ads-api/timezones\n",
    "# function to convert Twitter date string format\n",
    "# define the function\n",
    "\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "df = df.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates and retweets \n",
    "df = df.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates()\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "df = df.sort(\"post_created_at\", ascending=False)\n",
    "#removing spam accounts \n",
    "df = df.drop_duplicates([\"full_text\", \"screen_name\"])\n",
    "\n",
    "#df.printSchema()\n",
    "#df.count() #1340938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count hashtags\n",
    "def get_hashtags(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"#\" in word:\n",
    "            counter += 1\n",
    "    return(counter) \n",
    "\n",
    "# define function to count mentions\n",
    "def get_mentions(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"@\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count exclamation marks\n",
    "def get_exclamation_marks(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"!\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count number of emojis used\n",
    "import emojis\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)\n",
    "# register functions as udf\n",
    "get_hashtags_UDF = F.udf(get_hashtags, IntegerType())\n",
    "get_mentions_UDF = F.udf(get_mentions, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "emoji_counter_udf = F.udf(emoji_counter, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|lang|favorite_count|retweet_count|followers_count|    post_created_at|emoji_count|      text_tokenized|num_words|num_hashtags|num_mentions|num_exclamation_marks|\n",
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...| und|             6|            1|           4285|2022-05-14 00:55:33|          0|[!\\n#vegan, #GoVe...|        4|           3|           0|                    1|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|  en|             0|            0|            947|2022-01-15 07:17:18|          0|[!, We, will, be,...|       35|          21|           0|                    3|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|  en|             8|            3|          10745|2022-09-07 09:11:40|          0|[!!, Daily, Updat...|       31|           9|           2|                    2|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|  de|             2|            1|            148|2022-07-18 10:31:26|          0|[!B, Mal, gute, N...|       30|           2|           0|                    1|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|  en|             2|            3|            181|2022-02-06 13:48:33|          0|[!love, !iq, !wad...|       32|          14|           2|                    5|\n",
      "|       Meia noite üåÉ| joaopedro00021|Thu Dec 09 21:32:...|\" 'A√ßougue vegano...|  pt|             0|            0|            105|2021-12-09 21:32:21|          0|[\", 'A√ßougue, veg...|       19|           0|           0|                    0|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|  en|             0|            1|              2|2022-09-03 04:23:35|          0|[\", DID, YOU, KNO...|       18|          13|           0|                    0|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|  en|             0|            0|             87|2022-07-18 20:50:34|          0|[\", Nature's, Int...|       30|          21|           2|                    0|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|  en|             0|            1|           3547|2022-08-02 16:30:04|          0|[\", Scarlet, Puni...|       32|          21|           0|                    0|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|  de|             0|            1|              1|2022-08-04 14:36:52|          0|[\", Vegan, meinet...|       22|           1|           0|                    1|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|  in|             1|            0|            240|2022-08-01 15:44:25|          2|[\", nuna, \"\\n\", T...|       20|           0|           0|                    0|\n",
      "|    sukhkarm dhillon|SukhkarmDhillon|Sun Sep 11 16:31:...|\"  ú·¥Ä·¥ç  ô ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™...| und|             0|            0|            504|2022-09-11 16:31:24|          0|[\",  ú·¥Ä·¥ç,  ô, ·¥¢…™…¥·¥Ö…¢...|       13|           1|           0|                    0|\n",
      "|            VeganRoo|       VeganRoo|Sun Feb 06 08:36:...|\"#Dairy milk no l...|  en|             3|            1|           8744|2022-02-06 08:36:00|          0|[\"#Dairy, milk, n...|       36|          15|           0|                    0|\n",
      "|         Jill Beaven|     BeavenJill|Sat Apr 23 11:00:...|\"#Greenwashing is...|  en|             5|            4|             87|2022-04-23 11:00:17|          0|[\"#Greenwashing, ...|       35|           4|           1|                    0|\n",
      "|Rajan, for social...| BoycottPalmOil|Fri Mar 11 16:35:...|\"#Greenwashing is...|  en|             2|            2|            367|2022-03-11 16:35:07|          0|[\"#Greenwashing, ...|       35|           4|           1|                    0|\n",
      "|        Pauline Park|    paulinepark|Fri Feb 04 01:04:...|\"#NewYorkCity sch...|  en|             0|            1|           2390|2022-02-04 01:04:56|          0|[\"#NewYorkCity, s...|       40|           6|           0|                    0|\n",
      "|  üìñThat Wordy Oneüìö|  Herofthewords|Mon May 30 18:08:...|\"#Vegan food is d...|  en|            26|            6|           2457|2022-05-30 18:08:58|          0|[\"#Vegan, food, i...|        5|           1|           0|                    0|\n",
      "|               NiCHE|   NiCHE_Canada|Sat Jul 02 16:34:...|\"#Vegan food‚Äôs ab...|  en|             1|            2|           8289|2022-07-02 16:34:40|          0|[\"#Vegan, food‚Äôs,...|       37|           2|           0|                    0|\n",
      "|   Mercy For Animals|MercyForAnimals|Mon Mar 07 20:01:...|\"'My bad choleste...|  en|            55|           16|         308135|2022-03-07 20:01:00|          0|[\"'My, bad, chole...|       22|           0|           1|                    0|\n",
      "|What's On South W...|   tixSouthWest|Wed Jun 29 20:00:...|\"'Vegan Festival'...|  en|             0|            0|            538|2022-06-29 20:00:17|          0|[\"'Vegan, Festiva...|       25|           1|           0|                    0|\n",
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "twitter_df = df.withColumn(\"emoji_count\", emoji_counter_udf(\"full_text\")) \\\n",
    "                            .withColumn(\"text_tokenized\", F.split(\"full_text\", \" \")) \\\n",
    "                            .withColumn(\"num_words\", F.size(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_hashtags\", get_hashtags_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_mentions\", get_mentions_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_exclamation_marks\", get_exclamation_marks_UDF(\"text_tokenized\"))\n",
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emojis.decode(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words consisting out of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "\n",
    "    # return\n",
    "    return(cleaned_string) \n",
    "clean_text_udf = F.udf(clean_text, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc4ab2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = df.withColumn(\"text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter_df.where(twitter_df.lang == 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_not_eng = twitter_df.where(twitter_df.lang != 'en')\n",
    "# twitter_trans = pipeline.transform(twitter_not_eng)\n",
    "# twitter_trans = twitter_trans.withColumn(\"text\", concat_ws(\" \",twitter_trans.translation.result))\n",
    "# twitter_trans = twitter_trans.drop(\"translation\", \"document\",\"sentence\")\n",
    "# twitter_df = twitter_df.union(twitter_trans)\n",
    "# twitter_df = twitter_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from googletrans import Translator\n",
    "\n",
    "# translator = Translator() \n",
    "# translation = translator.translate(\"test\", dest='en')\n",
    "\n",
    "# def translate(tweet):\n",
    "#     if len(tweet)!=0:\n",
    "#         translator = Translator() \n",
    "#         translation = translator.translate(tweet, dest = 'en')\n",
    "#         return(translation.text)\n",
    "#     return None\n",
    "\n",
    "# translate_udf = F.udf(translate, StringType())\n",
    "\n",
    "# twitter_df = twitter_df.withColumn(\"translation\", translate_udf(\"text\"))\n",
    "\n",
    "# twitter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- followers_count: long (nullable = true)\n",
      " |-- post_created_at: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|lang|favorite_count|retweet_count|followers_count|    post_created_at|                text|\n",
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|  en|             0|            0|            947|2022-01-15 07:17:18|we will be open a...|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|  en|             8|            3|          10745|2022-09-07 09:11:40|daily updates tas...|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|  en|             2|            3|            181|2022-02-06 13:48:33|love iq waddup bu...|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|  en|             0|            1|              2|2022-09-03 04:23:35|did you know seno...|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|  en|             0|            0|             87|2022-07-18 20:50:34|natures interconn...|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|  en|             0|            1|           3547|2022-08-02 16:30:04|scarlet punica gr...|\n",
      "|            VeganRoo|       VeganRoo|Sun Feb 06 08:36:...|\"#Dairy milk no l...|  en|             3|            1|           8744|2022-02-06 08:36:00|dairy milk no lon...|\n",
      "|         Jill Beaven|     BeavenJill|Sat Apr 23 11:00:...|\"#Greenwashing is...|  en|             5|            4|             87|2022-04-23 11:00:17|greenwashing is r...|\n",
      "|Rajan, for social...| BoycottPalmOil|Fri Mar 11 16:35:...|\"#Greenwashing is...|  en|             2|            2|            367|2022-03-11 16:35:07|greenwashing is r...|\n",
      "|        Pauline Park|    paulinepark|Fri Feb 04 01:04:...|\"#NewYorkCity sch...|  en|             0|            1|           2390|2022-02-04 01:04:56|newyorkcity schoo...|\n",
      "|  üìñThat Wordy Oneüìö|  Herofthewords|Mon May 30 18:08:...|\"#Vegan food is d...|  en|            26|            6|           2457|2022-05-30 18:08:58|vegan food is dis...|\n",
      "|               NiCHE|   NiCHE_Canada|Sat Jul 02 16:34:...|\"#Vegan food‚Äôs ab...|  en|             1|            2|           8289|2022-07-02 16:34:40|vegan food‚Äôs abil...|\n",
      "|   Mercy For Animals|MercyForAnimals|Mon Mar 07 20:01:...|\"'My bad choleste...|  en|            55|           16|         308135|2022-03-07 20:01:00|my bad cholestero...|\n",
      "|What's On South W...|   tixSouthWest|Wed Jun 29 20:00:...|\"'Vegan Festival'...|  en|             0|            0|            538|2022-06-29 20:00:17|vegan festival ho...|\n",
      "|        Gayforbillie| MarleyRose0725|Tue Feb 08 04:51:...|\"(1) Don't be too...|  en|             1|            0|            119|2022-02-08 04:51:35|dont be too hard ...|\n",
      "|           RJ Buddha|INEya_veganfood|Fri Apr 22 13:47:...|\"(Steve 'Brexit')...|  en|             0|            0|            684|2022-04-22 13:47:07|steve brexit stev...|\n",
      "|Anarchist for üá∫?...|   intlibecosoc|Mon Jun 27 01:41:...|\"...it becomes a ...|  en|             0|            0|           2039|2022-06-27 01:41:55|it becomes questi...|\n",
      "|ùôèùôùùôöùòøùôöùôóùô§ùô£?...|       thee_sxs|Mon Nov 01 20:44:...|\"...or vegan ones...|  en|             1|            1|            119|2021-11-01 20:44:18|or vegan ones bra...|\n",
      "|             Krasnov|   greennomad61|Sun Mar 13 00:23:...|\".@Vita_Russia An...|  en|             0|            0|          14248|2022-03-13 00:23:54|vita russia anima...|\n",
      "|Luke Daniel Emers...|  Luke_Emerson1|Wed Mar 09 13:30:...|\"10% of surveyed ...|  en|             1|            0|           1036|2022-03-09 13:30:35|of surveyed swine...|\n",
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic Modelling\n",
    "\n",
    "https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP/blob/master/Topic_Modelling_with_PySpark_and_Spark_NLP.ipynb\n",
    "\n",
    "https://www.johnsnowlabs.com/spark-nlp/\n",
    "\n",
    "Topic modeling is a method for unsupervised classification of documents, similar to clustering on numeric data, which finds some natural groups of items (topics) even when we‚Äôre not sure what we‚Äôre looking for.\n",
    "\n",
    "Topic modeling provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives. \n",
    "It can help with the following:\n",
    "- discovering the hidden themes in the collection.\n",
    "- classifying the documents into the discovered themes.\n",
    "- using the classification to organize/summarize/search the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Spark NLP pipeline\n",
    "\n",
    "### 3.1.1 Basic NLP pipeline\n",
    "\n",
    "DocumentAssembler converts data into Spark NLP annotation format that can be used by Spark NLP annotators. Prepares data into a format that is processable by Spark NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotators\n",
    "\n",
    "Annotators are the spearhead of NLP functions in Spark NLP. There are two forms of annotators:\n",
    "\n",
    "Annotator Approaches: are those who represent a Spark ML Estimator and require a training stage. They have a function called fit(data) which trains a model based on some data. They produce the second type of annotator which is an annotator model or transformer.\n",
    "Annotator Models: are spark models or transformers, meaning they have a transform(data) function. This function takes as input a dataframe to which it adds a new column containing the result of the current annotation. All transformers are additive, meaning they append to current data, never replace or delete previous information.\n",
    "Both forms of annotators can be included in a Pipeline. All annotators included in a Pipeline will be automatically executed in the defined order and will transform the data accordingly. A Pipeline is turned into a PipelineModel after the fit() stage. The Pipeline can be saved to disk and re-loaded at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "                .setInputCol(\"text\") \\\n",
    "                .setOutputCol('document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tokenize the data with Tokenizer. Tokenizes raw text in document type columns into TokenizedSentence. Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean out the data and lower it with Normalizer. Annotator that cleans out tokens. Requires stems, hence tokens. Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Normalizer\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to lemmatize our text with pretrained lemming model provided by Spark NLP. We can access this model with LemmatizerModel.\n",
    "Class to find lemmas out of words with the objective of returning a base dictionary word. Retrieves the significant part of a word. A dictionary of predefined lemmas must be provided. The dictionary can be set as a delimited text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907,6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907,6 KB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import LemmatizerModel\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemmatized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark NLP doesn't provide a stop word list, hence, we will use nltk package to download stop words for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import StopWordsCleaner\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to unigrams, it is good to use n-grams for topic modelling as well since they help to better refine topics. We can get n-grams with NGramGenerator in Spark NLP. N-grams try to predict which word will have the highest probability of appearing with the other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import NGramGenerator\n",
    "\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['lemmatized']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have our basic NLP pipeline for topic modelling with all necessary steps. However, let's use POS tagger in order to improve our processed data for topic modelling even more with POS tagged data later. For this, we are going to use pretrained POS tagging model provided by Spark NLP. We can access the model with PerceptronModel. Trains an averaged Perceptron model to tag words part-of-speech. Sets a POS tag to each word within a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3,9 MB\n",
      "[ | ]pos_anc download started this may take some time.\n",
      "Approximate size to download 3,9 MB\n",
      "[ / ]Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import PerceptronModel\n",
    "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
    "     .setInputCols(['document', 'lemmatized']) \\\n",
    "     .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything in Spark NLP annotation format. To be able to process the data further, we need to tranform data with Finisher. Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. The Finisher outputs annotation(s) values into String.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['unigrams', 'ngrams', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to input everything into a pipeline. Pipeline functionality is accessible with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline() \\\n",
    "     .setStages([documentAssembler,\n",
    "                 tokenizer,\n",
    "                 normalizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 pos_tagger,\n",
    "                 ngrammer,\n",
    "                 finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = pipeline.fit(twitter_df).transform(twitter_df)\n",
    "processed_review = processed_review.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, screen_name: string, created_at: string, lang: string, favorite_count: bigint, retweet_count: bigint, followers_count: bigint, post_created_at: timestamp, finished_unigrams: array<string>, finished_ngrams: array<string>, finished_pos: array<string>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_review.drop(\"full_text\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=====================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            name|    screen_name|          created_at|           full_text|lang|favorite_count|retweet_count|followers_count|    post_created_at|                text|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
      "+----------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     Jill Beaven|     BeavenJill|Sat Apr 23 11:00:...|\"#Greenwashing is...|  en|             5|            4|             87|2022-04-23 11:00:17|greenwashing is r...|[greenwashing, ri...|[greenwashing, be...|[VBG, VB, NN, IN,...|\n",
      "|        The Beet|thebeetofficial|Tue Nov 02 18:33:...|\"A lot of people ...|  en|             4|            0|           1977|2021-11-02 18:33:05|lot of people say...|[lot, people, say...|[lot, of, people,...|[NN, IN, NNS, VBP...|\n",
      "|ISA Study Abroad|      ISAabroad|Thu Mar 10 14:30:...|\"By now, your [ve...|  en|             0|            0|          16873|2022-03-10 14:30:04|by now your vegan...|[veganvegetarian,...|[by, now, you, ve...|[IN, RB, PRP, JJ,...|\n",
      "|             kev|     bitkevcoin|Tue Apr 26 18:00:...|\"Don't stay in th...|  en|             3|            0|            814|2022-04-26 18:00:01|dont stay in the ...|[dont, stay, sun,...|[dont, stay, in, ...|[NN, NN, IN, DT, ...|\n",
      "| Rob Whitehall ‚ìã|   robdoubleyoo|Tue Jun 21 11:34:...|\"Heroes sacrifice...|  en|            36|           28|          19704|2022-06-21 11:34:53|heroes sacrifice ...|[hero, sacrifice,...|[hero, sacrifice,...|[NN, NN, IN, NN, ...|\n",
      "+----------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Extended NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have our data in a form of unigrams that are lemmatized, with no stop words in there. I think it is a good idea to incorporate n-grams into our NLP pipeline. We obtained n-grams as one step of our pipeline but now n-grams are messy and have a lot of questionable combinations in there. To tackle this problem, let's filter out strange combinations of words in n-grams based on their POS tags. We can imagine a list of viable combinations like ADJ + NOUN so let's restrict our POS combinations in n-grams to this list. Plus, we can also exclude some POS tags from our unigrams to ensure that we don't use functional words for topic modelling (they can be partially covered by stop words but probably not fully).\n",
    "\n",
    "Doing this POS-based filtering will significantly reduce the vocabulary size for topic modelling which will speed up the whole processing.\n",
    "\n",
    "Let's start this processing. First, we need to join all our POS tags obtained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start another Spark NLP pipeline in order to get POS tag n-grams that correspond to word n-grams. We start with convertation into Spark NLP annotation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('finished_pos') \\\n",
    "     .setOutputCol('pos_document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tokenize our POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['pos_document']) \\\n",
    "     .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate n-grams from them in the same way we did that for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['pos']) \\\n",
    "    .setOutputCol('pos_ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we are ready to get POS tags ngrams with Finisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_finisher = Finisher() \\\n",
    "     .setInputCols(['pos', 'pos_ngrams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create this new Spark NLP pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pipeline = Pipeline() \\\n",
    "     .setStages([pos_documentAssembler,                  \n",
    "                 pos_tokenizer,\n",
    "                 pos_ngrammer,  \n",
    "                 pos_finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and again fit it and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look what kind of data we have to operate with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'screen_name',\n",
       " 'created_at',\n",
       " 'full_text',\n",
       " 'lang',\n",
       " 'favorite_count',\n",
       " 'retweet_count',\n",
       " 'followers_count',\n",
       " 'post_created_at',\n",
       " 'text',\n",
       " 'finished_unigrams',\n",
       " 'finished_ngrams',\n",
       " 'finished_pos',\n",
       " 'finished_pos_ngrams']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_review = processed_review.cache()\n",
    "processed_review.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are our word n-grams with their corresponding pos n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:====================================================> (193 + 7) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|     finished_ngrams| finished_pos_ngrams|\n",
      "+--------------------+--------------------+\n",
      "|[greenwashing, be...|[VBG, VB, NN, IN,...|\n",
      "|[lot, of, people,...|[NN, IN, NNS, VBP...|\n",
      "|[by, now, you, ve...|[IN, RB, PRP, JJ,...|\n",
      "|[dont, stay, in, ...|[NN, NN, IN, DT, ...|\n",
      "|[hero, sacrifice,...|[NN, NN, IN, NN, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('finished_ngrams', 'finished_pos_ngrams').limit(5).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to filter out not useful for topic modelling analysis POS tags from our data. Let's create the function that does it for unigrams first. We create the custom Python function and then transform it to PySpark UDF to be used on Spark dataframe. What are the POS tags?\n",
    "\n",
    "- NN is singular noun\n",
    "- NNS is plural noun\n",
    "- VB is verb\n",
    "- VBP verb, present tense not 3rd person singular(wrap)\n",
    "- JJ is an adjective (large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
    "\n",
    "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply this function on columns with unigrams and their POS tags to get filtered unigrams in a separate dataframe column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_unigrams',\n",
    "                                               udf_filter_pos(F.col('finished_unigrams'), \n",
    "                                                              F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how our filtered unigrams look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:=================================================>    (182 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                         filtered_unigrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[rife, product, product, although, load, corruption, around, palm, oil, boycottpalmoil,...|\n",
      "|                                       [lot, say, cant, vegan, im, like, lewis, eat, loss]|\n",
      "|                               [easy, maintain, even, itall, study, hannahs, stay, abroad]|\n",
      "|[dont, stay, sunscreen, red, meat, bad, vegan, diet, healthy, healthy, inflation, good,...|\n",
      "|                  [hero, sacrifice, thing, may, great, wont, hide, animalliberation, love]|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_unigrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to filter out improper POS combinations of n-grams. We create the custom function in the same manner as before. Since we deal with bi- and trigrams, we need to restrict tags for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos_combs(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if (len(pos.split('_')) == 2 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
    "            or (len(pos.split('_')) == 3 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
    "    \n",
    "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we call the function on word and POS n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_ngrams',\n",
    "                                               udf_filter_pos_combs(F.col('finished_ngrams'),\n",
    "                                                                    F.col('finished_pos_ngrams'))).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what we get after filtering for n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:====================================================> (195 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                           filtered_ngrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[be_rife, buy_label, label_product, be_load, include_palm, palm_oil, boycottpalmoil_boy...|\n",
      "|[cant_eat, eat_vegan, nichole_lewis, be_yearold, yearold_mom, mom_lose, lose_pound, wei...|\n",
      "|                                      [veganvegetarian_lifestyle, hannahs_tip, stay_vegan]|\n",
      "|[dont_stay, sun_use, use_sunscreen, sunscreen_red, red_meat, be_bad, vegan_diet, be_hea...|\n",
      "|[hero_sacrifice, do_thing, other_hide, great_hero, brandon_mull, mull_antispeciesism, a...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_ngrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have unigrams and n-grams stored in different columns in the dataframe. Let's combine them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "processed_review = processed_review.withColumn('final', \n",
    "                                               concat(F.col('filtered_unigrams'), \n",
    "                                                      F.col('filtered_ngrams'))).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is our final look of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 77:=================================================>    (184 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                                     final|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[rife, product, product, although, load, corruption, around, palm, oil, boycottpalmoil,...|\n",
      "|[lot, say, cant, vegan, im, like, lewis, eat, loss, cant_eat, eat_vegan, nichole_lewis,...|\n",
      "|[easy, maintain, even, itall, study, hannahs, stay, abroad, veganvegetarian_lifestyle, ...|\n",
      "|[dont, stay, sunscreen, red, meat, bad, vegan, diet, healthy, healthy, inflation, good,...|\n",
      "|[hero, sacrifice, thing, may, great, wont, hide, animalliberation, love, hero_sacrifice...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('final').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_remove\n",
    "\n",
    "processed_review = processed_review.withColumn(\"final\", array_remove(\"final\", \"vegan\"))\n",
    "processed_review = processed_review.withColumn(\"final\", array_remove(\"final\", \"amp\"))\n",
    "processed_review = processed_review.withColumn(\"final\", array_remove(\"final\", \"food\"))\n",
    "processed_review = processed_review.withColumn(\"final\", array_remove(\"final\", \"be_vegan\"))\n",
    "processed_review = processed_review.withColumn(\"final\", array_remove(\"final\", \"go\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are set to vectorization of our data. First, we will proceed with TF (term frequency) vectorization with CountVectorizer in PySpark. We fit tf dictionary and then transform the data to vectors of counts.\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
    "tf_model = tfizer.fit(processed_review)\n",
    "tf_result = tf_model.transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get TF results, we can account for words that are frequent for all the documents. We can use IDF (inverse document frequency) to lower score of such words.\n",
    "\n",
    "The inverse document frequency is a measure of whether a term is common or rare in a given document corpus. It is obtained by dividing the total number of documents by the number of documents containing the term in the corpus.\n",
    "\n",
    "While computing TF, all terms are considered equally important. However it is known that certain terms, such as ‚Äúis‚Äù, ‚Äúof‚Äù, and ‚Äúthat‚Äù, may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing IDF, an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n",
    "IDF is the inverse of the document frequency which measures the informativeness of term t. When we calculate IDF, it will be very low for the most occurring words such as stop words (because stop words such as ‚Äúis‚Äù is present in almost all of the documents, and N/df will give a very low value to that word). This finally gives what we want, a relative weightage.\n",
    "\n",
    "Now there are few other problems with the IDF , in case of a large corpus,say 100,000,000 , the IDF value explodes , to avoid the effect we take the log of idf .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Ftf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558&psig=AOvVaw2NphvHAPexM_UP_4UAcgLP&ust=1670086684586000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCJjOxO6z2_sCFQAAAAAdAAAAABAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:04:34 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 96:====================================================> (196 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:04:42 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2\n",
    "\n",
    "Finally, we are ready to model topics in our data with LDA (Latent Dirichlet Allocation). To use the algorithm, we have to provide the number of topics we presume our data contains and the number of iterations for the LDA algorithm. Then, we initialize the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:04:43 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|lang|favorite_count|retweet_count|followers_count|    post_created_at|                text|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|   filtered_unigrams|     filtered_ngrams|               final|         tf_features|     tf_idf_features|\n",
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|         Jill Beaven|     BeavenJill|Sat Apr 23 11:00:...|\"#Greenwashing is...|  en|             5|            4|             87|2022-04-23 11:00:17|greenwashing is r...|[greenwashing, ri...|[greenwashing, be...|[VBG, VB, NN, IN,...|[VBG, VB, NN, IN,...|[rife, product, p...|[be_rife, buy_lab...|[rife, product, p...|(262144,[53,124,5...|(262144,[53,124,5...|\n",
      "|            The Beet|thebeetofficial|Tue Nov 02 18:33:...|\"A lot of people ...|  en|             4|            0|           1977|2021-11-02 18:33:05|lot of people say...|[lot, people, say...|[lot, of, people,...|[NN, IN, NNS, VBP...|[NN, IN, NNS, VBP...|[lot, say, cant, ...|[cant_eat, eat_ve...|[lot, say, cant, ...|(262144,[3,4,12,3...|(262144,[3,4,12,3...|\n",
      "|    ISA Study Abroad|      ISAabroad|Thu Mar 10 14:30:...|\"By now, your [ve...|  en|             0|            0|          16873|2022-03-10 14:30:04|by now your vegan...|[veganvegetarian,...|[by, now, you, ve...|[IN, RB, PRP, JJ,...|[IN, RB, PRP, JJ,...|[easy, maintain, ...|[veganvegetarian_...|[easy, maintain, ...|(262144,[101,110,...|(262144,[101,110,...|\n",
      "|                 kev|     bitkevcoin|Tue Apr 26 18:00:...|\"Don't stay in th...|  en|             3|            0|            814|2022-04-26 18:00:01|dont stay in the ...|[dont, stay, sun,...|[dont, stay, in, ...|[NN, NN, IN, DT, ...|[NN, NN, IN, DT, ...|[dont, stay, suns...|[dont_stay, sun_u...|[dont, stay, suns...|(262144,[9,10,13,...|(262144,[9,10,13,...|\n",
      "|     Rob Whitehall ‚ìã|   robdoubleyoo|Tue Jun 21 11:34:...|\"Heroes sacrifice...|  en|            36|           28|          19704|2022-06-21 11:34:53|heroes sacrifice ...|[hero, sacrifice,...|[hero, sacrifice,...|[NN, NN, IN, NN, ...|[NN, NN, IN, NN, ...|[hero, sacrifice,...|[hero_sacrifice, ...|[hero, sacrifice,...|(262144,[15,82,84...|(262144,[15,82,84...|\n",
      "|‚ö°Ô∏èüëë Princess D.J...| NewOceaniaArmy|Wed Mar 09 21:53:...|\"It's ok to eat a...|  en|             0|            0|             29|2022-03-09 21:53:18|its ok to eat and...|[ok, eat, experim...|[it, ok, to, eat,...|[PRP, JJ, TO, VB,...|[PRP, JJ, TO, VB,...|[eat, pig, cant, ...|[pig_cant, cant_d...|[eat, pig, cant, ...|(262144,[3,33,113...|(262144,[3,33,113...|\n",
      "|  FAB #Veggie #Vegan| fabveggievegan|Sat Apr 30 18:58:...|\"Last year over a...|  en|             0|            2|          25024|2022-04-30 18:58:29|last year over mi...|[last, year, mill...|[last, year, over...|[JJ, NN, IN, CD, ...|[JJ, NN, IN, CD, ...|[last, year, leav...|[last_year, same_...|[last, year, leav...|(262144,[43,50,58...|(262144,[43,50,58...|\n",
      "|KaLena Bowers üìñ?...|   BowersKaLena|Mon Dec 13 18:22:...|\"Monday Mint Moti...|  en|            14|           19|           7929|2021-12-13 18:22:14|monday mint motiv...|[monday, mint, mo...|[monday, mint, mo...|[NNP, NN, NN, NN,...|[NNP, NN, NN, NN,...|[mint, motivation...|[mint_motivation,...|[mint, motivation...|(262144,[40,66,10...|(262144,[40,66,10...|\n",
      "|Haruka - Former C...|    HarukaChamp|Tue Nov 02 20:54:...|\"No nut... Novemb...|  en|             1|            0|            214|2021-11-02 20:54:27|no nut november m...|[nut, november, m...|[no, nut, novembe...|[DT, NN, NNP, VB,...|[DT, NN, NNP, VB,...|[november, love, ...|[mean_love, love_...|[november, love, ...|(262144,[9,15,842...|(262144,[9,15,842...|\n",
      "|GhostWipe - Premi...|    _GhostWipe_|Fri Apr 29 18:40:...|\"Sir, why did you...|  en|             0|            0|              2|2022-04-29 18:40:00|sir why did you s...|[sir, start, laug...|[sir, why, do, yo...|[NN, WRB, VBP, PR...|[NN, WRB, VBP, PR...|[sir, laugh, defe...|[start_laugh, squ...|[sir, laugh, defe...|(262144,[292,600,...|(262144,[292,600,...|\n",
      "|   Quadram Institute|     TheQuadram|Tue Jan 11 20:05:...|\"There are many g...|  en|             3|            2|           8708|2022-01-11 20:05:01|there are many go...|[many, good, reas...|[there, be, many,...|[EX, VB, JJ, JJ, ...|[EX, VB, JJ, JJ, ...|[good, reason, fo...|[be_many, many_go...|[good, reason, fo...|(262144,[8,9,17,4...|(262144,[8,9,17,4...|\n",
      "|           MetroMedi|     MetroMedi1|Wed Apr 27 07:09:...|\"Vegan Diets\" pro...|  en|             0|            0|            871|2022-04-27 07:09:44|vegan diets provi...|[vegan, diet, pro...|[vegan, diet, pro...|[NN, NN, VBP, DT,...|[NN, NN, VBP, DT,...|[vegan, diet, pro...|[vegan_diet, heal...|[diet, provide, h...|(262144,[1,17,24,...|(262144,[1,17,24,...|\n",
      "|   Admiral Keetonyan|      Keetonman|Wed Aug 03 23:27:...|\"Veganism is heal...|  en|             3|            0|             64|2022-08-03 23:27:22|veganism is healt...|[veganism, health...|[veganism, be, he...|[NN, VB, JJ, CD, ...|[NN, VB, JJ, CD, ...|[veganism, health...|[be_healthy, prot...|[veganism, health...|(262144,[2,10,19,...|(262144,[2,10,19,...|\n",
      "|     Jamie Woodhouse| JamieWoodhouse|Sun Jul 31 16:54:...|\"We were given a ...|  en|             3|            0|           8278|2022-07-31 16:54:40|we were given veg...|[give, vegan, see...|[we, be, give, ve...|[PRP, VB, VB, NN,...|[PRP, VB, VB, NN,...|[vegan, seedling,...|[give_vegan, god_...|[seedling, diet, ...|(262144,[17,37,50...|(262144,[17,37,50...|\n",
      "|     Rob Whitehall ‚ìã|   robdoubleyoo|Wed Aug 10 16:53:...|\"Why are vegans s...|  en|            53|           71|          19902|2022-08-10 16:53:36|why are vegans so...|[vegan, annoy, an...|[why, be, vegan, ...|[WRB, VB, NNS, RB...|[WRB, VB, NNS, RB...|[annoy, animal, h...|[be_vegan, be_ani...|[annoy, animal, h...|(262144,[1,7,16,1...|(262144,[1,7,16,1...|\n",
      "|              Riley!| PrivateMeRiley|Tue Aug 02 17:00:...|\"Would you ever g...|  en|             0|            0|            227|2022-08-02 17:00:38|would you ever go...|[would, ever, go,...|[would, you, ever...|[MD, PRP, RB, VBP...|[MD, PRP, RB, VBP...|[vegetarian, vega...|[go_vegetarian, v...|[vegetarian, love...|(262144,[14,15,34...|(262144,[14,15,34...|\n",
      "|                 Jay|   Chxinsawenby|Mon Nov 01 21:57:...|\"You doing anythi...|  en|             8|            0|            455|2021-11-01 21:57:48|you doing anythin...|[anything, specia...|[you, do, anythin...|[PRP, VBP, NN, JJ...|[PRP, VBP, NN, JJ...|[special, world, ...|[do_anything, any...|[special, world, ...|(262144,[18,65,19...|(262144,[18,65,19...|\n",
      "|         BND For All|BigNonVeganDave|Tue Mar 08 18:19:...|\"defeating Russia...|  en|             4|            1|            857|2022-03-08 18:19:53|defeating russia ...|[defeat, russia, ...|[defeat, russia, ...|[NN, NNP, VB, JJ,...|[NN, NNP, VB, JJ,...|[defeat, pointles...|[be_pointless, uk...|[defeat, pointles...|(262144,[1903,142...|(262144,[1903,142...|\n",
      "|Erlijn van Genuch...|        ErlijnG|Thu Mar 31 12:30:...|#365sustainableDe...|  en|            44|           38|          96169|2022-03-31 12:30:00|sustainabledecisi...|[sustainabledecis...|[sustainabledecis...|[NNS, VBP, VB, NN...|[NNS, VBP, VB, NN...|[sustainabledecis...|[prepare_vegan, v...|[sustainabledecis...|(262144,[79,98,33...|(262144,[79,98,33...|\n",
      "|   Apparel Resources|    apparel_res|Wed Jun 29 12:30:...|#ARfeatures - Cai...|  en|             0|            0|           4268|2022-06-29 12:30:39|arfeatures cai st...|[arfeatures, cai,...|[arfeatures, cai,...|[NNS, NN, NN, VBN...|[NNS, NN, NN, VBN...|[arfeatures, cai,...|[arfeatures_cai, ...|[arfeatures, cai,...|(262144,[45,96,15...|(262144,[45,96,15...|\n",
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:04:43 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:04:54 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:04:54 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:04:55 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:04:55 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/12/08 11:04:55 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 117:====================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:07 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:09 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:05:09 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 126:=================================================>   (188 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:17 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:18 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:05:19 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 135:==================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:23 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:24 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:05:25 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 144:==================================================>  (189 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:29 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:31 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:05:31 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 153:==================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:38 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:40 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:05:40 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 162:==================================================>  (190 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:46 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:48 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:05:49 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 171:===================================================> (195 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:53 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:05:55 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:05:55 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 180:==================================================>  (191 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:06:00 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:06:02 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:06:02 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 189:===================================================> (194 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:06:07 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:06:09 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/12/08 11:06:09 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 198:===================================================> (193 + 7) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:06:14 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "num_topics = 3\n",
    "max_iter = 10\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features').setTopicDistributionCol(\"topicDistributionCol\")\n",
    "lda_model = lda.fit(tfidf_result)\n",
    "transformed = lda_model.transform(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:06:16 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n",
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|lang|favorite_count|retweet_count|followers_count|    post_created_at|                text|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|   filtered_unigrams|     filtered_ngrams|               final|         tf_features|     tf_idf_features|topicDistributionCol|\n",
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|         Jill Beaven|     BeavenJill|Sat Apr 23 11:00:...|\"#Greenwashing is...|  en|             5|            4|             87|2022-04-23 11:00:17|greenwashing is r...|[greenwashing, ri...|[greenwashing, be...|[VBG, VB, NN, IN,...|[VBG, VB, NN, IN,...|[rife, product, p...|[be_rife, buy_lab...|[rife, product, p...|(262144,[53,124,5...|(262144,[53,124,5...|[0.20748682873590...|\n",
      "|            The Beet|thebeetofficial|Tue Nov 02 18:33:...|\"A lot of people ...|  en|             4|            0|           1977|2021-11-02 18:33:05|lot of people say...|[lot, people, say...|[lot, of, people,...|[NN, IN, NNS, VBP...|[NN, IN, NNS, VBP...|[lot, say, cant, ...|[cant_eat, eat_ve...|[lot, say, cant, ...|(262144,[3,4,12,3...|(262144,[3,4,12,3...|[0.33335055071088...|\n",
      "|    ISA Study Abroad|      ISAabroad|Thu Mar 10 14:30:...|\"By now, your [ve...|  en|             0|            0|          16873|2022-03-10 14:30:04|by now your vegan...|[veganvegetarian,...|[by, now, you, ve...|[IN, RB, PRP, JJ,...|[IN, RB, PRP, JJ,...|[easy, maintain, ...|[veganvegetarian_...|[easy, maintain, ...|(262144,[101,110,...|(262144,[101,110,...|[0.99167873408332...|\n",
      "|                 kev|     bitkevcoin|Tue Apr 26 18:00:...|\"Don't stay in th...|  en|             3|            0|            814|2022-04-26 18:00:01|dont stay in the ...|[dont, stay, sun,...|[dont, stay, in, ...|[NN, NN, IN, DT, ...|[NN, NN, IN, DT, ...|[dont, stay, suns...|[dont_stay, sun_u...|[dont, stay, suns...|(262144,[9,10,13,...|(262144,[9,10,13,...|[0.37833810022459...|\n",
      "|     Rob Whitehall ‚ìã|   robdoubleyoo|Tue Jun 21 11:34:...|\"Heroes sacrifice...|  en|            36|           28|          19704|2022-06-21 11:34:53|heroes sacrifice ...|[hero, sacrifice,...|[hero, sacrifice,...|[NN, NN, IN, NN, ...|[NN, NN, IN, NN, ...|[hero, sacrifice,...|[hero_sacrifice, ...|[hero, sacrifice,...|(262144,[15,82,84...|(262144,[15,82,84...|[0.18215075668171...|\n",
      "|‚ö°Ô∏èüëë Princess D.J...| NewOceaniaArmy|Wed Mar 09 21:53:...|\"It's ok to eat a...|  en|             0|            0|             29|2022-03-09 21:53:18|its ok to eat and...|[ok, eat, experim...|[it, ok, to, eat,...|[PRP, JJ, TO, VB,...|[PRP, JJ, TO, VB,...|[eat, pig, cant, ...|[pig_cant, cant_d...|[eat, pig, cant, ...|(262144,[3,33,113...|(262144,[3,33,113...|[0.21145876359466...|\n",
      "|  FAB #Veggie #Vegan| fabveggievegan|Sat Apr 30 18:58:...|\"Last year over a...|  en|             0|            2|          25024|2022-04-30 18:58:29|last year over mi...|[last, year, mill...|[last, year, over...|[JJ, NN, IN, CD, ...|[JJ, NN, IN, CD, ...|[last, year, leav...|[last_year, same_...|[last, year, leav...|(262144,[43,50,58...|(262144,[43,50,58...|[0.42141356457623...|\n",
      "|KaLena Bowers üìñ?...|   BowersKaLena|Mon Dec 13 18:22:...|\"Monday Mint Moti...|  en|            14|           19|           7929|2021-12-13 18:22:14|monday mint motiv...|[monday, mint, mo...|[monday, mint, mo...|[NNP, NN, NN, NN,...|[NNP, NN, NN, NN,...|[mint, motivation...|[mint_motivation,...|[mint, motivation...|(262144,[40,66,10...|(262144,[40,66,10...|[0.18362239006791...|\n",
      "|Haruka - Former C...|    HarukaChamp|Tue Nov 02 20:54:...|\"No nut... Novemb...|  en|             1|            0|            214|2021-11-02 20:54:27|no nut november m...|[nut, november, m...|[no, nut, novembe...|[DT, NN, NNP, VB,...|[DT, NN, NNP, VB,...|[november, love, ...|[mean_love, love_...|[november, love, ...|(262144,[9,15,842...|(262144,[9,15,842...|[0.00674087249087...|\n",
      "|GhostWipe - Premi...|    _GhostWipe_|Fri Apr 29 18:40:...|\"Sir, why did you...|  en|             0|            0|              2|2022-04-29 18:40:00|sir why did you s...|[sir, start, laug...|[sir, why, do, yo...|[NN, WRB, VBP, PR...|[NN, WRB, VBP, PR...|[sir, laugh, defe...|[start_laugh, squ...|[sir, laugh, defe...|(262144,[292,600,...|(262144,[292,600,...|[0.07432675586203...|\n",
      "|   Quadram Institute|     TheQuadram|Tue Jan 11 20:05:...|\"There are many g...|  en|             3|            2|           8708|2022-01-11 20:05:01|there are many go...|[many, good, reas...|[there, be, many,...|[EX, VB, JJ, JJ, ...|[EX, VB, JJ, JJ, ...|[good, reason, fo...|[be_many, many_go...|[good, reason, fo...|(262144,[8,9,17,4...|(262144,[8,9,17,4...|[0.57523465605265...|\n",
      "|           MetroMedi|     MetroMedi1|Wed Apr 27 07:09:...|\"Vegan Diets\" pro...|  en|             0|            0|            871|2022-04-27 07:09:44|vegan diets provi...|[vegan, diet, pro...|[vegan, diet, pro...|[NN, NN, VBP, DT,...|[NN, NN, VBP, DT,...|[vegan, diet, pro...|[vegan_diet, heal...|[diet, provide, h...|(262144,[1,17,24,...|(262144,[1,17,24,...|[0.30493985176099...|\n",
      "|   Admiral Keetonyan|      Keetonman|Wed Aug 03 23:27:...|\"Veganism is heal...|  en|             3|            0|             64|2022-08-03 23:27:22|veganism is healt...|[veganism, health...|[veganism, be, he...|[NN, VB, JJ, CD, ...|[NN, VB, JJ, CD, ...|[veganism, health...|[be_healthy, prot...|[veganism, health...|(262144,[2,10,19,...|(262144,[2,10,19,...|[0.00722266404847...|\n",
      "|     Jamie Woodhouse| JamieWoodhouse|Sun Jul 31 16:54:...|\"We were given a ...|  en|             3|            0|           8278|2022-07-31 16:54:40|we were given veg...|[give, vegan, see...|[we, be, give, ve...|[PRP, VB, VB, NN,...|[PRP, VB, VB, NN,...|[vegan, seedling,...|[give_vegan, god_...|[seedling, diet, ...|(262144,[17,37,50...|(262144,[17,37,50...|[0.53337322132922...|\n",
      "|     Rob Whitehall ‚ìã|   robdoubleyoo|Wed Aug 10 16:53:...|\"Why are vegans s...|  en|            53|           71|          19902|2022-08-10 16:53:36|why are vegans so...|[vegan, annoy, an...|[why, be, vegan, ...|[WRB, VB, NNS, RB...|[WRB, VB, NNS, RB...|[annoy, animal, h...|[be_vegan, be_ani...|[annoy, animal, h...|(262144,[1,7,16,1...|(262144,[1,7,16,1...|[0.34254275070020...|\n",
      "|              Riley!| PrivateMeRiley|Tue Aug 02 17:00:...|\"Would you ever g...|  en|             0|            0|            227|2022-08-02 17:00:38|would you ever go...|[would, ever, go,...|[would, you, ever...|[MD, PRP, RB, VBP...|[MD, PRP, RB, VBP...|[vegetarian, vega...|[go_vegetarian, v...|[vegetarian, love...|(262144,[14,15,34...|(262144,[14,15,34...|[0.63079164306922...|\n",
      "|                 Jay|   Chxinsawenby|Mon Nov 01 21:57:...|\"You doing anythi...|  en|             8|            0|            455|2021-11-01 21:57:48|you doing anythin...|[anything, specia...|[you, do, anythin...|[PRP, VBP, NN, JJ...|[PRP, VBP, NN, JJ...|[special, world, ...|[do_anything, any...|[special, world, ...|(262144,[18,65,19...|(262144,[18,65,19...|[0.00489172485568...|\n",
      "|         BND For All|BigNonVeganDave|Tue Mar 08 18:19:...|\"defeating Russia...|  en|             4|            1|            857|2022-03-08 18:19:53|defeating russia ...|[defeat, russia, ...|[defeat, russia, ...|[NN, NNP, VB, JJ,...|[NN, NNP, VB, JJ,...|[defeat, pointles...|[be_pointless, uk...|[defeat, pointles...|(262144,[1903,142...|(262144,[1903,142...|[0.00693946715938...|\n",
      "|Erlijn van Genuch...|        ErlijnG|Thu Mar 31 12:30:...|#365sustainableDe...|  en|            44|           38|          96169|2022-03-31 12:30:00|sustainabledecisi...|[sustainabledecis...|[sustainabledecis...|[NNS, VBP, VB, NN...|[NNS, VBP, VB, NN...|[sustainabledecis...|[prepare_vegan, v...|[sustainabledecis...|(262144,[79,98,33...|(262144,[79,98,33...|[0.50066357553377...|\n",
      "|   Apparel Resources|    apparel_res|Wed Jun 29 12:30:...|#ARfeatures - Cai...|  en|             0|            0|           4268|2022-06-29 12:30:39|arfeatures cai st...|[arfeatures, cai,...|[arfeatures, cai,...|[NNS, NN, NN, VBN...|[NNS, NN, NN, VBN...|[arfeatures, cai,...|[arfeatures_cai, ...|[arfeatures, cai,...|(262144,[45,96,15...|(262144,[45,96,15...|[0.58910557584255...|\n",
      "+--------------------+---------------+--------------------+--------------------+----+--------------+-------------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(input_list):\n",
    "    max = input_list[0]\n",
    "    index = 0\n",
    "    for i in range(1,len(input_list)):\n",
    "        if input_list[i] > max:\n",
    "            max = input_list[i]\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "get_topic_udf = udf(lambda z: get_topic(z), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transformed.withColumn('topic', get_topic_udf(\"topicDistributionCol\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:06:17 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 207:====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:06:49 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 212:===================>                                     (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:06:50 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n",
      "22/12/08 11:06:51 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:06:51 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n"
     ]
    }
   ],
   "source": [
    "freq_month = transformed.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "freq_month = freq_month.withColumn(\"month\", month(df[\"post_created_at\"]))\n",
    "\n",
    "freq_month = freq_month.groupBy('year', 'month', 'topic').agg(countDistinct(\"full_text\"))\\\n",
    "               .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                    .sort('year', 'month', ascending = True)\n",
    "freq_month = freq_month.select(concat_ws('_',freq_month.year, freq_month.month)\\\n",
    "                            .alias('date'), 'topic', 'freq').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plotly.com"
       },
       "data": [
        {
         "hoverongaps": true,
         "type": "heatmap",
         "x": [
          "2021_10",
          "2021_10",
          "2021_10",
          "2021_11",
          "2021_11",
          "2021_11",
          "2021_12",
          "2021_12",
          "2021_12",
          "2022_1",
          "2022_1",
          "2022_1",
          "2022_2",
          "2022_2",
          "2022_2",
          "2022_3",
          "2022_3",
          "2022_3",
          "2022_4",
          "2022_4",
          "2022_4",
          "2022_5",
          "2022_5",
          "2022_5",
          "2022_6",
          "2022_6",
          "2022_6",
          "2022_7",
          "2022_7",
          "2022_7",
          "2022_8",
          "2022_8",
          "2022_8",
          "2022_9",
          "2022_9",
          "2022_9",
          "2022_10",
          "2022_10",
          "2022_10"
         ],
         "y": [
          2,
          1,
          0,
          1,
          2,
          0,
          2,
          0,
          1,
          1,
          0,
          2,
          1,
          0,
          2,
          2,
          1,
          0,
          0,
          2,
          1,
          2,
          0,
          1,
          0,
          2,
          1,
          2,
          1,
          0,
          2,
          0,
          1,
          2,
          1,
          0,
          2,
          0,
          1
         ],
         "z": [
          5912,
          10108,
          12345,
          14460,
          5701,
          12522,
          10612,
          22894,
          20919,
          9712,
          9733,
          6132,
          10329,
          9223,
          5428,
          12801,
          26975,
          29228,
          17866,
          7574,
          15848,
          5743,
          9744,
          10391,
          14031,
          6720,
          14258,
          10722,
          22472,
          24349,
          10096,
          25359,
          22367,
          3193,
          5294,
          4445,
          664,
          856,
          1172
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                   x=freq_month[\"date\"],\n",
    "                   y=freq_month[\"topic\"],\n",
    "                   z=freq_month[\"freq\"],\n",
    "                   hoverongaps = True))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:06:54 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 235:====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:07:22 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:07:22 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n",
      "22/12/08 11:07:22 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n"
     ]
    }
   ],
   "source": [
    "transformed = transformed.withColumn('eng_rate', ((transformed['favorite_count'] + transformed['retweet_count'])/transformed['followers_count']))\n",
    "eng_topic = transformed.groupBy('topic').agg(avg(\"eng_rate\")) \\\n",
    "                        .withColumnRenamed(\"avg(eng_rate)\", \"eng_rate\") \\\n",
    "                        .sort('eng_rate', ascending = False).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>eng_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.038672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.032845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic  eng_rate\n",
       "0      0  0.038672\n",
       "1      1  0.033318\n",
       "2      2  0.032845"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 16:13:02 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 910073 ms exceeds timeout 120000 ms\n",
      "22/12/08 16:13:02 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "eng_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:07:23 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 255:====================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:07:55 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 260:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:07:56 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 11:07:56 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n",
      "22/12/08 11:07:57 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>192536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>184257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>91257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic    freq\n",
       "0      0  192536\n",
       "1      1  184257\n",
       "2      2   91257"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_topic = transformed.groupBy('topic').agg(countDistinct(\"full_text\")) \\\n",
    "                        .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                        .sort('freq', ascending = False).toPandas()\n",
    "count_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>2</td>\n",
       "      <td>5912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>1</td>\n",
       "      <td>10108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>0</td>\n",
       "      <td>12345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021_11</td>\n",
       "      <td>1</td>\n",
       "      <td>14460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021_11</td>\n",
       "      <td>2</td>\n",
       "      <td>5701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021_11</td>\n",
       "      <td>0</td>\n",
       "      <td>12522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021_12</td>\n",
       "      <td>2</td>\n",
       "      <td>10612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021_12</td>\n",
       "      <td>0</td>\n",
       "      <td>22894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021_12</td>\n",
       "      <td>1</td>\n",
       "      <td>20919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022_1</td>\n",
       "      <td>1</td>\n",
       "      <td>9712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022_1</td>\n",
       "      <td>2</td>\n",
       "      <td>6132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022_2</td>\n",
       "      <td>1</td>\n",
       "      <td>10329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022_2</td>\n",
       "      <td>0</td>\n",
       "      <td>9223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022_2</td>\n",
       "      <td>2</td>\n",
       "      <td>5428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022_3</td>\n",
       "      <td>2</td>\n",
       "      <td>12801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022_3</td>\n",
       "      <td>1</td>\n",
       "      <td>26975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022_3</td>\n",
       "      <td>0</td>\n",
       "      <td>29228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022_4</td>\n",
       "      <td>0</td>\n",
       "      <td>17866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022_4</td>\n",
       "      <td>2</td>\n",
       "      <td>7574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022_4</td>\n",
       "      <td>1</td>\n",
       "      <td>15848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022_5</td>\n",
       "      <td>2</td>\n",
       "      <td>5743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022_5</td>\n",
       "      <td>0</td>\n",
       "      <td>9744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022_5</td>\n",
       "      <td>1</td>\n",
       "      <td>10391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022_6</td>\n",
       "      <td>0</td>\n",
       "      <td>14031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2022_6</td>\n",
       "      <td>2</td>\n",
       "      <td>6720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022_6</td>\n",
       "      <td>1</td>\n",
       "      <td>14258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2022_7</td>\n",
       "      <td>2</td>\n",
       "      <td>10722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2022_7</td>\n",
       "      <td>1</td>\n",
       "      <td>22472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2022_7</td>\n",
       "      <td>0</td>\n",
       "      <td>24349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2022_8</td>\n",
       "      <td>2</td>\n",
       "      <td>10096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2022_8</td>\n",
       "      <td>0</td>\n",
       "      <td>25359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2022_8</td>\n",
       "      <td>1</td>\n",
       "      <td>22367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2022_9</td>\n",
       "      <td>2</td>\n",
       "      <td>3193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2022_9</td>\n",
       "      <td>1</td>\n",
       "      <td>5294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022_9</td>\n",
       "      <td>0</td>\n",
       "      <td>4445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>2</td>\n",
       "      <td>664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>0</td>\n",
       "      <td>856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  topic   freq\n",
       "0   2021_10      2   5912\n",
       "1   2021_10      1  10108\n",
       "2   2021_10      0  12345\n",
       "3   2021_11      1  14460\n",
       "4   2021_11      2   5701\n",
       "5   2021_11      0  12522\n",
       "6   2021_12      2  10612\n",
       "7   2021_12      0  22894\n",
       "8   2021_12      1  20919\n",
       "9    2022_1      1   9712\n",
       "10   2022_1      0   9733\n",
       "11   2022_1      2   6132\n",
       "12   2022_2      1  10329\n",
       "13   2022_2      0   9223\n",
       "14   2022_2      2   5428\n",
       "15   2022_3      2  12801\n",
       "16   2022_3      1  26975\n",
       "17   2022_3      0  29228\n",
       "18   2022_4      0  17866\n",
       "19   2022_4      2   7574\n",
       "20   2022_4      1  15848\n",
       "21   2022_5      2   5743\n",
       "22   2022_5      0   9744\n",
       "23   2022_5      1  10391\n",
       "24   2022_6      0  14031\n",
       "25   2022_6      2   6720\n",
       "26   2022_6      1  14258\n",
       "27   2022_7      2  10722\n",
       "28   2022_7      1  22472\n",
       "29   2022_7      0  24349\n",
       "30   2022_8      2  10096\n",
       "31   2022_8      0  25359\n",
       "32   2022_8      1  22367\n",
       "33   2022_9      2   3193\n",
       "34   2022_9      1   5294\n",
       "35   2022_9      0   4445\n",
       "36  2022_10      2    664\n",
       "37  2022_10      0    856\n",
       "38  2022_10      1   1172"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(469429, 22)\n"
     ]
    }
   ],
   "source": [
    "print((transformed.count(), len(transformed.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to see words that characterize the defined topics, we need to convert word ids into actual words with the custom function. This function will again be converted to PySpark UDF to be used on our topic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the number of top words per topic we would like to see and extract the words with our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                topicWords|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|    0|[im, make, eat, veganism, get, like, dont, heart, recipe, think, meat, one, diet, healt...|\n",
      "|    1|[heart, animal, make, eat, go_vegan, green, plantbased, day, recipe, diet, good, vegani...|\n",
      "|    2|[soap, organic, check, beauty, collision, healthy, recipe, vegan_beauty, plantbased, sp...|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 15\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 290:>                                                        (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "im\n",
      "make\n",
      "eat\n",
      "veganism\n",
      "get\n",
      "like\n",
      "dont\n",
      "heart\n",
      "recipe\n",
      "think\n",
      "meat\n",
      "one\n",
      "diet\n",
      "healthy\n",
      "visit\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "heart\n",
      "animal\n",
      "make\n",
      "eat\n",
      "go_vegan\n",
      "green\n",
      "plantbased\n",
      "day\n",
      "recipe\n",
      "diet\n",
      "good\n",
      "veganism\n",
      "seedling\n",
      "im\n",
      "vegetarian\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "soap\n",
      "organic\n",
      "check\n",
      "beauty\n",
      "collision\n",
      "healthy\n",
      "recipe\n",
      "vegan_beauty\n",
      "plantbased\n",
      "spice\n",
      "fitness\n",
      "atheist\n",
      "heart\n",
      "love\n",
      "gift\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e35d98e8198887147a5837b6820e4bf8d41831f6222e06e86b8679b6549872f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
